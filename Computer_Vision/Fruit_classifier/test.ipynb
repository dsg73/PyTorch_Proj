{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2.post2\n",
      "0.16.1\n"
     ]
    }
   ],
   "source": [
    "# Import Pytorch\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Import torchvision\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check versions\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "# Define transformation to apply to the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),  # Resize images to 224x224\n",
    "    transforms.ToTensor(),           # Convert images to tensors\n",
    "])\n",
    "\n",
    "# Define paths to your train and validation data\n",
    "train_path = 'train'\n",
    "val_path = 'validation'\n",
    "\n",
    "# Load the train and validation datasets using ImageFolder\n",
    "train_data = ImageFolder(root=train_path, transform=transform)\n",
    "test_data = ImageFolder(root=val_path, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41322, 13877)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.9922, 0.9961, 0.9922,  ..., 1.0000, 0.9922, 1.0000],\n",
       "          [0.9922, 1.0000, 0.9922,  ..., 0.9961, 0.9922, 1.0000],\n",
       "          [0.9961, 0.9961, 0.9961,  ..., 0.9922, 0.9961, 1.0000],\n",
       "          ...,\n",
       "          [0.9961, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 0.9961, 0.9961,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       " \n",
       "         [[0.9961, 0.9961, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.9961, 0.9961, 1.0000,  ..., 0.9961, 1.0000, 1.0000],\n",
       "          [0.9922, 1.0000, 0.9961,  ..., 0.9961, 0.9961, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 0.9961,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.9961, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       " \n",
       "         [[0.9961, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 0.9961, 0.9922,  ..., 0.9961, 0.9961, 1.0000],\n",
       "          [1.0000, 0.9922, 0.9961,  ..., 0.9961, 0.9961, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 0.9961,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 0.9961,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]]),\n",
       " 0)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image,label = train_data[2]\n",
    "image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apple Braeburn',\n",
       " 'Apple Golden 1',\n",
       " 'Apple Golden 2',\n",
       " 'Apple Golden 3',\n",
       " 'Apple Granny Smith',\n",
       " 'Apple Red 1',\n",
       " 'Apple Red 2',\n",
       " 'Apple Red 3',\n",
       " 'Apple Red Delicious',\n",
       " 'Apple Red Yellow',\n",
       " 'Apricot',\n",
       " 'Avocado',\n",
       " 'Avocado ripe',\n",
       " 'Banana',\n",
       " 'Banana Red',\n",
       " 'Cactus fruit',\n",
       " 'Cantaloupe 1',\n",
       " 'Cantaloupe 2',\n",
       " 'Carambula',\n",
       " 'Cherry 1',\n",
       " 'Cherry 2',\n",
       " 'Cherry Rainier',\n",
       " 'Cherry Wax Black',\n",
       " 'Cherry Wax Red',\n",
       " 'Cherry Wax Yellow',\n",
       " 'Clementine',\n",
       " 'Cocos',\n",
       " 'Dates',\n",
       " 'Granadilla',\n",
       " 'Grape Pink',\n",
       " 'Grape White',\n",
       " 'Grape White 2',\n",
       " 'Grapefruit Pink',\n",
       " 'Grapefruit White',\n",
       " 'Guava',\n",
       " 'Huckleberry',\n",
       " 'Kaki',\n",
       " 'Kiwi',\n",
       " 'Kumquats',\n",
       " 'Lemon',\n",
       " 'Lemon Meyer',\n",
       " 'Limes',\n",
       " 'Lychee',\n",
       " 'Mandarine',\n",
       " 'Mango',\n",
       " 'Maracuja',\n",
       " 'Melon Piel de Sapo',\n",
       " 'Mulberry',\n",
       " 'Nectarine',\n",
       " 'Orange',\n",
       " 'Papaya',\n",
       " 'Passion Fruit',\n",
       " 'Peach',\n",
       " 'Peach Flat',\n",
       " 'Pear',\n",
       " 'Pear Abate',\n",
       " 'Pear Monster',\n",
       " 'Pear Williams',\n",
       " 'Pepino',\n",
       " 'Physalis',\n",
       " 'Physalis with Husk',\n",
       " 'Pineapple',\n",
       " 'Pineapple Mini',\n",
       " 'Pitahaya Red',\n",
       " 'Plum',\n",
       " 'Pomegranate',\n",
       " 'Quince',\n",
       " 'Rambutan',\n",
       " 'Raspberry',\n",
       " 'Salak',\n",
       " 'Strawberry',\n",
       " 'Strawberry Wedge',\n",
       " 'Tamarillo',\n",
       " 'Tangelo',\n",
       " 'Tomato 1',\n",
       " 'Tomato 2',\n",
       " 'Tomato 3',\n",
       " 'Tomato 4',\n",
       " 'Tomato Cherry Red',\n",
       " 'Tomato Maroon',\n",
       " 'Walnut']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = train_data.classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([3, 28, 28]) -> [color_channels, height, width]\n",
      "Image Label: shape: Apple Braeburn \n"
     ]
    }
   ],
   "source": [
    "# Check the shape of our images\n",
    "print(f\"Image shape: {image.shape} -> [color_channels, height, width]\")\n",
    "print(f\"Image Label: shape: {class_names[label]} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape:torch.Size([3, 28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '0')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "image, label = train_data[0]\n",
    "print(f\"Image shape:{image.shape}\")\n",
    "image = image.permute(1, 2, 0)\n",
    "plt.imshow(image)\n",
    "plt.title(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset ImageFolder\n",
       "     Number of datapoints: 41322\n",
       "     Root location: train\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                Resize(size=(28, 28), interpolation=bilinear, max_size=None, antialias=warn)\n",
       "                ToTensor()\n",
       "            ),\n",
       " Dataset ImageFolder\n",
       "     Number of datapoints: 13877\n",
       "     Root location: validation\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                Resize(size=(28, 28), interpolation=bilinear, max_size=None, antialias=warn)\n",
       "                ToTensor()\n",
       "            ))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set the batch size hyperparameter\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Turn datasets into iterables (batches)\n",
    "train_dataloader = DataLoader(dataset = train_data,\n",
    "                              batch_size = BATCH_SIZE,\n",
    "                              shuffle = True)\n",
    "test_dataloader = DataLoader(dataset = test_data,\n",
    "                              batch_size = BATCH_SIZE,\n",
    "                              shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloader: (<torch.utils.data.dataloader.DataLoader object at 0x17ff71cd0>, <torch.utils.data.dataloader.DataLoader object at 0x2c24dc9d0>) \n",
      "Length of train_dataloader: 646 batches of 64...\n",
      "Length of train_dataloader: 217 batches of 64...\n"
     ]
    }
   ],
   "source": [
    "# lets check what we've created\n",
    "print(f\"Dataloader: {train_dataloader, test_dataloader} \")\n",
    "print(f\"Length of train_dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}...\")\n",
    "print(f\"Length of train_dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Device  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to calculate time elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "def print_train_time(start:float,\n",
    "                     end: float,\n",
    "                     device: torch.device = None):\n",
    "  \"\"\"Print diff bet start and end time\"\"\"\n",
    "  total_time = end- start\n",
    "  print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
    "  return total_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Training and Testing Loop as train_step and test_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module,\n",
    "               data_loader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               accuracy_fn,\n",
    "               device: torch.device = device):\n",
    "  \"\"\"Performs a training with model trying to learn on data_loader\"\"\"\n",
    "  train_loss, train_acc = 0,0\n",
    "  model.train()\n",
    "\n",
    "  # Add a loop to loop through the training batches\n",
    "  for batch, (X, y) in enumerate(data_loader):\n",
    "    # Put data on target device\n",
    "    X,y = X.to(device), y.to(device)\n",
    "\n",
    "    # 1. Forward pass\n",
    "    y_pred = model(X)\n",
    "\n",
    "    # 2. Calculate loss and accuracy (per batch)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    train_loss += loss # accumulate train loss\n",
    "    train_acc += accuracy_fn(y_true = y,\n",
    "                             y_pred = y_pred.argmax(dim=1))\n",
    "\n",
    "    # 3. Optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Loss backward\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "  # Divide total train_loss  and acc by length of train dataloader\n",
    "  train_loss /= len(data_loader)\n",
    "  train_acc /= len(data_loader)\n",
    "  print(f\"Train loss: {train_loss:.5f} | Train acc:{train_acc:.2f}% \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model: torch.nn.Module,\n",
    "              data_loader: torch.utils.data.DataLoader,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              accuracy_fn,\n",
    "              device: torch.device = device):\n",
    "\n",
    "  \"\"\"Performs a testing loop step on model going over data_loader.\"\"\"\n",
    "\n",
    "  test_loss, test_acc = 0,0\n",
    "\n",
    "  model.eval()\n",
    "\n",
    "  with torch.inference_mode():\n",
    "    for X, y in data_loader:\n",
    "       # Put data to device\n",
    "       X, y = X.to(device), y.to(device)\n",
    "\n",
    "       # 1. Forward pass\n",
    "       test_pred = model(X)\n",
    "\n",
    "       # 2. Calcualte loss\n",
    "       test_loss += loss_fn(test_pred, y)\n",
    "\n",
    "       # 3. Caculate accuracy\n",
    "       test_acc += accuracy_fn(y_true = y, y_pred = test_pred.argmax(dim=1))\n",
    "\n",
    "    # Calculate the test loss avg per batch\n",
    "    test_loss /= len(data_loader)\n",
    "\n",
    "\n",
    "    # Calculate the test acc avg per batch\n",
    "    test_acc /= len(data_loader)\n",
    "\n",
    "    # pinrt out\n",
    "    print(f\"\\n Test loss: {test_loss:.5f} | Test acc: {test_acc:.2f}% \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define our Custom Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FruitClassifierModel(nn.Module):\n",
    "  \"\"\"\n",
    "  Model architecure that replicates the TinyVGG\n",
    "  Model from CNN EXPLAINER\n",
    "  \"\"\"\n",
    "  def __init__(self,\n",
    "               input_shape: int,\n",
    "               hidden_units: int,\n",
    "               output_shape:int):\n",
    "    super().__init__()\n",
    "\n",
    "    self.conv_block_1 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels = input_shape,\n",
    "                  out_channels = hidden_units,\n",
    "                  kernel_size = 3,\n",
    "                  stride = 1,\n",
    "                  padding = 1), # values we can set ourselves in our NN's are called hyperparameter\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels = hidden_units,\n",
    "                  out_channels = hidden_units,\n",
    "                  kernel_size = 3,\n",
    "                  stride = 1,\n",
    "                  padding = 1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size = 2)\n",
    "    )\n",
    "\n",
    "    self.conv_block_2 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels = hidden_units,\n",
    "                  out_channels = hidden_units,\n",
    "                  kernel_size = 3,\n",
    "                  stride = 1,\n",
    "                  padding = 1), # values we can set ourselves in our NN's are called hyperparameter\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels = hidden_units,\n",
    "                  out_channels = hidden_units,\n",
    "                  kernel_size = 3,\n",
    "                  stride = 1,\n",
    "                  padding = 1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size = 2)\n",
    "    )\n",
    "    \n",
    "    self.classifier = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(in_features = hidden_units*7*7, # there's a trick to calculate this\n",
    "                  out_features = output_shape),\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv_block_1(x)\n",
    "    # print(x.shape)\n",
    "    x = self.conv_block_2(x)\n",
    "    # print(x.shape)\n",
    "    # x = self.conv_block_3(x)\n",
    "    # print(x.shape)\n",
    "    x = self.classifier(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "model_1 = FruitClassifierModel(input_shape = 3, # no. of channels of images\n",
    "                               hidden_units=50,\n",
    "                               output_shape = len(class_names)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup loss and optimizer and evaluation metrics\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params = model_1.parameters(),\n",
    "                            lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 \n",
      "-------\n",
      "Train loss: 3.49675 | Train acc:17.85% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:27<00:55, 27.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test loss: 2.19268 | Test acc: 41.15% \n",
      "\n",
      "Epoch:1 \n",
      "-------\n",
      "Train loss: 0.39920 | Train acc:89.81% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [00:52<00:25, 25.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test loss: 0.51764 | Test acc: 88.57% \n",
      "\n",
      "Epoch:2 \n",
      "-------\n",
      "Train loss: 0.05482 | Train acc:98.83% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [01:16<00:00, 25.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test loss: 0.50161 | Test acc: 89.12% \n",
      "\n",
      "Train time on mps:0: 76.196 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Import tqdm for progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# set the seed and start the time\n",
    "torch.manual_seed(42)\n",
    "train_time_start_on_gpu = timer()\n",
    "\n",
    "# Set the no. of epochs\n",
    "epochs = 3\n",
    "\n",
    "# Create optimization and evaluation loop using train_step and test_step\n",
    "for epoch in tqdm(range(epochs)):\n",
    "  print(f\"Epoch:{epoch} \\n-------\")\n",
    "  ### Training\n",
    "  train_step(model = model_1,\n",
    "             data_loader = train_dataloader,\n",
    "             loss_fn = loss_fn,\n",
    "             optimizer = optimizer,\n",
    "             accuracy_fn=accuracy_fn,\n",
    "             device=device)\n",
    "\n",
    "  ### Testing\n",
    "  test_step(model = model_1,\n",
    "            data_loader = test_dataloader,\n",
    "            loss_fn = loss_fn,\n",
    "            accuracy_fn=accuracy_fn,\n",
    "            device=device)\n",
    "\n",
    "# calculate the training time\n",
    "train_time_end_on_gpu = timer()\n",
    "total_train_time_model_1 = print_train_time(start = train_time_start_on_gpu,\n",
    "                                            end = train_time_end_on_gpu,\n",
    "                                            device = str(next(model_1.parameters()).device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:11<00:00, 19.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_name': 'FruitClassifierModel',\n",
       " 'model_loss': 0.501611053943634,\n",
       " 'model_acc': 89.12010368663594}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "def eval_model(model:torch.nn.Module,\n",
    "               data_loader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               accuracy_fn,\n",
    "               device: torch.device = device ):\n",
    "  \"\"\" Returna dictionary containing the results of model predicting on data_loader\"\"\"\n",
    "  loss, acc = 0,0\n",
    "  model.eval()\n",
    "  with torch.inference_mode():\n",
    "    for X,y in tqdm(data_loader):\n",
    "      # Put data to device\n",
    "      X,y = X.to(device), y.to(device)\n",
    "\n",
    "      # make prediction\n",
    "      y_pred = model(X)\n",
    "\n",
    "      # Accumulate the loss and acc values per batch\n",
    "      loss += loss_fn(y_pred, y)\n",
    "      acc += accuracy_fn(y_true=y,\n",
    "                         y_pred = y_pred.argmax(dim =1 ))\n",
    "\n",
    "    # Scale loss and acc to find the avg loss/acc per batch\n",
    "    loss /= len(data_loader)\n",
    "    acc /= len(data_loader)\n",
    "\n",
    "\n",
    "  return {\"model_name\": model.__class__.__name__, # only works when model was created with class}\n",
    "          \"model_loss\": loss.item(),\n",
    "          \"model_acc\": acc}\n",
    "\n",
    "# Calculate model 0 resutlst on test dataset\n",
    "\n",
    "model_1_results = eval_model(model= model_1,\n",
    "                             data_loader = test_dataloader,\n",
    "                             loss_fn = loss_fn,\n",
    "                             accuracy_fn = accuracy_fn)\n",
    "\n",
    "model_1_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stable_diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
